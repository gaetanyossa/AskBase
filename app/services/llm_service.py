from langchain_openai import ChatOpenAI
import json
from jinja2 import Template
import logging

logger = logging.getLogger(__name__)

class LLMService:
    def __init__(self, api_key: str):
        logger.info("ğŸ§  Initialisation de LLMService avec GPT-4")
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0,
            openai_api_key=api_key
        )
        with open("app/prompts/structured_prompt.txt", "r", encoding="utf-8") as f:
            self.template = Template(f.read())

    def build_prompt(self, user_question: str, dataset: str, schema: list, selected_table: str) -> str:
        """
        Construit un prompt structurÃ© en se basant uniquement sur la table sÃ©lectionnÃ©e.
        """
        # Filtrer pour ne garder que la table sÃ©lectionnÃ©e
        table_info = next((t for t in schema if t["name"] == selected_table), None)

        if not table_info:
            raise ValueError(f"La table sÃ©lectionnÃ©e '{selected_table}' n'a pas Ã©tÃ© trouvÃ©e dans le schÃ©ma.")

        # GÃ©nÃ©rer le prompt structurÃ© avec les bonnes variables
        prompt_text = self.template.render(
            prompt=user_question,
            dataset=dataset,
            selected_table=selected_table,
            tables=[table_info]  # Une seule table dans une liste
        )

        #logger.info(f"ğŸ§  Prompt structurÃ© :\n{prompt_text}")
        return prompt_text




    def query(self, structured_prompt: str) -> dict:
        try:
            response = self.llm.invoke(structured_prompt)
            logger.debug(f"ğŸ§  RÃ©ponse brute : {response.content!r}")
            result = json.loads(response.content)
            logger.info("âœ… RÃ©ponse du LLM reÃ§ue et parsÃ©e")
            return result
        except Exception as e:
            logger.error(f"âŒ Erreur dans la requÃªte LLM : {e}")
            raise
